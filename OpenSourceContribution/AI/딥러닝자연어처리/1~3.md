

[딥러닝자연어처리-MinsukHeo허민석](https://www.youtube.com/playlist?list=PLVNY1HnUlO26qqZznHVWAqjS1fWw0zqnT)

# Bag of Words

## Bag of Words란 무엇인가?

![](https://velog.velcdn.com/images/narae/post/19d4b259-ddb7-4607-9c9f-670ad29a5eeb/image.png)

not bad not good이란 말을 가방에 넣어본다고 하자, 그럼, not한개, bad한개, good한개, Eh not한개가 들어갈것이다

- 단어의 출연 빈도로 이 문장을 숫자로 나타내면..
    
    ![](https://velog.velcdn.com/images/narae/post/63891a47-da8b-49b8-b41c-8cfbbf9724cf/image.png)
    

**[1,1,1,0,0,0]**

> Bag of Words
> 
> 
> 문장을 숫자로 표현하는 방식 중 하나’
> 

### 근데 이렇게 수치화 시켜서 어디다가 써먹지?

!https://velog.velcdn.com/images/narae/post/cd324ccb-95e6-4a99-a708-3c08305bb666/image.png

**1.문장의 유사도를 판단할 때 사용할 수 있다.**

EX)awesome thank you 와 great thank you 사이의 유사도 판단하기

- 각각의 인덱스에있는 숫자끼리 곱하고 이것을 더한다

유사도 : 1+1=2

**2.머신러닝 모델에 입력값으로 사용할 수 있다.**

!https://velog.velcdn.com/images/narae/post/e62db705-f38d-4c6b-a92a-baca00492816/image.png

머신러닝 모델들은 모두 수학적인 모델이기 때문에, 문장으로 입력할 수 없고 수치화 시켜서 인식시켜야한다.

## 단점

### 1. Sparsity

!https://velog.velcdn.com/images/narae/post/c23ea254-3f66-49d1-816d-03a54d820f8a/image.png

백만개가 넘는 단어들을 넣는다면..?

- > 문장을 숫자료 표현하면 0이 무수히 많을 것이고 0이 아닌 숫자들을 매우 적을 것이다.
- --->계산량이 높아지고 메모리도 많이 사용된다.

### 2. Frequent words has more power

!https://velog.velcdn.com/images/narae/post/b60b13f2-1c9c-4c32-b101-ade7aa0d6eec/image.png

자주 사용하는 단어들이 더 파워를 가지게 된다. 위 사진을 보면, the man like the girl 과 the man love the girl사이의 유사도보다, thethethe사이의 유사도가 더 높은 것을 볼 수 있다.

### Ignoring word orders

!https://velog.velcdn.com/images/narae/post/783af6da-9faf-4ecb-ae73-6ce53ac750e9/image.png

단어의 순서를 완전히 무시해버린다.

- home run VS run home홈런을 치다와 집으로 달리다는 매우 다른 뜻이지만, home run과 run home둘다 run이 1번 사용되고 home이 1번 사용되어 똑같은 벡터로 나타내짐

### Out of vocabulary

!https://velog.velcdn.com/images/narae/post/c13f310e-fe10-4e65-b124-026477a6536c/image.png

데이터상 없는 단어의 경우 나타낼 수가 없다. (줄임말, 신조어, 약간의 오타와 같은 경우 인식불가)

---

# What is N gram?

!https://velog.velcdn.com/images/narae/post/45a44a4c-93a2-43e3-89ca-c0454a4c6b44/image.png

N-gram은 연속적인 N개의 토큰들을 얘기한다. 자연어처리에서 토큰은 단어나 또는 캐릭터로 판단한다. 단어나 기본 로직은 bag of words랑 비슷한데 n개의 글자로 글자를 쪼개서 판단하는것

### 1-gram(unigram)

!https://velog.velcdn.com/images/narae/post/e883eaa5-a8c8-4e36-a16f-e7935f46f232/image.png

각각의 캐릭터가 하나의 토큰으로 구분된다

### 2-gram (bigram)

https://velog.velcdn.com/images/narae/post/70d5ef6d-08fe-479b-87a0-c91b63a97df6/image.png

두개의 캐릭터가 하나의 묶음이됐당~

### 3-gram (tigram)

!(https://velog.velcdn.com/images/narae/post/bc5360eb-059e-45e2-87b2-9eaafb5ab1d6/image.png)

세개의 캐릭터가 하나의 토큰이 됐다~

## 왜 n-gram을 사용하는가!

- bag of words가 단어의 순서를 무시하는 단점을 어느정도 극복할수있음
- 다음에 올 단어를 예측할 수 있음
- 오타를 찾을 수 있음
    
    등등..
    

## bag of words' drawback

**machine learning is fun and is not boring**

- **bag of words 사용했을 때는 이렇게 표현됨**

!https://velog.velcdn.com/images/narae/post/32e5554a-631b-45f1-a20b-92b0825b8c67/image.png

> not이 어디에 속한지 모르는 bag of words
> 
> 
> 순서가 무시되었기 때문에 fun이 not이라고 하는지.. boring이 not인지 모르는 백오브월드.
> 
- **근데!! bag of bigram 사용하면?!**

!https://velog.velcdn.com/images/narae/post/98d757f7-d0af-433f-a810-f4b2deecf280/image.png

> not boring 이 토큰으로 묶였기 때문에 not이 boring에 속한다는 걸 알 수 있당
> 

## Naive Next word prediction

!https://velog.velcdn.com/images/narae/post/e0864228-67b9-4afd-8fd4-61615b88f404/image.png

**사용자가 how are 까지 쳤을 때...**

how are 다음에 you가 온게 2개있었고 they가 온게 1개이니 you를 추천해 줄 수있는것임

(진보적인 방법은 아님)

## Naive Spell checker

![](https://velog.velcdn.com/images/narae/post/dd011248-0f7e-4dc7-87dd-13e573870ad9/image.png)

**사용자가 만약 ... qwal이라고 쳤을때...**

논리 데이터셋에 qw가 없음 대신 qu많이 발생 -> 오타라고 인식하고 qu를 추천해줄수있음

---

# What is TF-IDF

> TF-IDF
> 
> 
> Term Frequency*Inverse Document Frequency
> 

## 왜 TF-IDF를 사용하나요?

문서에는 단어가 있을 것이고 각 단어별로 문서별 연관성을 찾을 때 사용. 각 단어별로 이 문서에 대한 정보를 얼마만큼 가지고있는가를 수치로 낸것임.

## **TF(Term Frequency)**

각 문서마다 이 단어가 몇번 출현했는가?

문서가 있을 때 그 단어가 많이 출현하면 출현할 수록 연관성이 높을 것이다.

TF score EX)

[]!(https://velog.velcdn.com/images/narae/post/f5b9cdb4-3989-43a1-bc09-33dbb0d34a76/image.png)

a의 TF score은 1/7

car의 TF score 3/7

이문장에서 가장 높은 TF score을 가진 것은 car -> 이문장에서 가장 중요한 단어는 Car이다.

### TF score drawback ![](https://velog.velcdn.com/images/narae/post/85dbe4da-0e63-4fe4-a4bb-0c76fec380e1/image.png)

a와 friend가 똑같은 스코어를 가짐 (공동 1등) but a는 중요하지않다.

이와같이 a나 the와 같이 자주 사용되는 정관사의 경우 중요하지는 않지만 tf score가 높게나올수있음

## What is IDF?

[]!(https://velog.velcdn.com/images/narae/post/6969c4b8-ce57-4cc9-bfaa-d569b3c0b35e/image.png)

어느 문장에서나 많이 나오는 단어(ex. a,the)에 대해서 패널티를 주기위해서 IDF라는 개념이 탄생. 공식은 Log안에서 총문장의 개수를 문장에 나온 개수로 나눠주는 것. 때로는 +1을 하여 0으로 나누는 것을 방지하고자함

- IDF score
    
    []!(https://velog.velcdn.com/images/narae/post/bc7ac1eb-9c6b-42e8-abea-574cd5e23a83/image.png)
    

**a**

log( 문장이 총 2개이니까 2 / a는 두문장 둘다 있으니까 2 ) = 0

**new**

log( 두문장 2 / 한문장에서만 출현했으니 1) = 0.3

# TF-IDF score

[]!(https://velog.velcdn.com/images/narae/post/85c03f8a-277e-4883-9fd8-758a48bb4865/image.png)

A에서 가장 높은 TF*IDF 값 0.13을 가진 car라는 단어가 가장 연관성있다고 판단B에서는 가장 높은 TF*IDF값 0.08을 가진 friend가 가장 연관성 있다고 판단

a는 0으로 연관성이 없다고 판단될 수 있음.